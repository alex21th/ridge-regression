{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MATHEMATICAL OPTIMIZATION\n",
    "**Bachelor Degree in Data Science and Engineering** - Alex Carrillo Alza, Xavier Rubiés Cullell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\LARGE \\textsf{Lab assignment 2}\n",
    "$$\n",
    "$$\n",
    "\\huge \\textsf{Ridge regression in Python}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment\n",
    "\n",
    "- Implement and solve in Python the constrained ridge regression model:\n",
    "$$\n",
    "\\begin{array}{rl}        \\displaystyle   \\min_{w,\\gamma} & \\frac{1}{2} (Aw+\\gamma-y)^{\\top} (Aw+\\gamma-y)\\\\        \\hbox{s. to} & \\|w\\|_2^2 \\le t      \\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "- You have to implement code for the evaluation of:\n",
    "$$f(x), \\nabla f(x),  \\nabla^2 f(x), h_i(x), \\nabla h_i(x),  \\nabla^2 h_i(x), i=1,\\dots,m$$\n",
    "\n",
    "- Try your implementation with the attached `death_rate` dataset.\n",
    "\n",
    "\n",
    "- Check that you obtain the same result as with the AMPL implementation  of ridge regression.\n",
    "\n",
    "\n",
    "- The report must include all the previous elements (python code, results obtained, analysis of results, etc).\n",
    "\n",
    "\n",
    "- This assignment is to be done in groups of two students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The ridge or Tikhonov-regularized regression is based on finding an affine function, $y = w^{\\top}x + \\gamma, \\quad w \\in \\mathbb R^n, \\gamma \\in \\mathbb R$, which solves the **constrained problem**\n",
    "\n",
    "$$\n",
    "\\begin{array}{rl}        \\displaystyle   \\min_{w,\\gamma} & \\frac{1}{2} (Aw+\\gamma e-y)^{\\top} (Aw+\\gamma e-y)\\\\        \\hbox{s. to} & \\|w\\|_2^2 \\le t      \\end{array}\n",
    "$$\n",
    "\n",
    "> An alternative (but different) *unconstrained* ridge regression model is (for some parameter $\\mu$)\n",
    "\n",
    "$$\n",
    "\\min_{w,\\gamma} \\frac{1}{2} (Aw+\\gamma e-y)^{\\top} (Aw+\\gamma e-y) + \\mu(\\|w\\|_2^2 - t)\n",
    "$$\n",
    "\n",
    "> Ridge regression is numerically better than standard regression (e.g., if $A^{\\top}A$ is close to singular)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preliminary calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We are given $m$ points $(x_i,y_i), i = 1,\\dots,m$, where $x_i \\in \\mathbb R^n, y \\in \\mathbb R$. Define matrix $A$ and vector $y$\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix} x_1^{\\top} \\\\\n",
    "                    \\vdots \\\\\n",
    "                    x_m^{\\top}\n",
    "\\end{bmatrix}\n",
    "\\qquad\n",
    "y = \\begin{bmatrix} y_1 \\\\\n",
    "                    \\vdots \\\\\n",
    "                    y_m\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Considering $\\mathbf{f(x)} := f(w, \\gamma)$ and $\\mathbf{h(x)} := h(w, \\gamma)$ where $w \\in \\mathbb R^n, \\gamma \\in \\mathbb R$, define\n",
    "\n",
    "$$\n",
    "w = \\begin{bmatrix} w_1 & \\cdots & w_n \\end{bmatrix}^{\\top}\n",
    "\\qquad\n",
    "\\gamma \\in \\mathbb R\n",
    "\\qquad\n",
    "e = \\begin{bmatrix} 1 & \\overset{m}{\\dotsb} & 1 \\end{bmatrix}^{\\top}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We get the following functions, gradients and hessians:\n",
    "\n",
    "$$\n",
    "\\mathbf{f(x)} = \\frac{1}{2} (Aw+\\gamma e-y)^{\\top} (Aw+\\gamma e-y) \\in \\mathbb R\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{\\mathbf{\\nabla f(x)} = \\begin{bmatrix} \\frac{\\partial f}{\\partial w} \\\\\n",
    "\\frac{\\partial f}{\\partial \\gamma}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix} A^{\\top}(Aw + \\gamma e - y) \\\\\n",
    "e^{\\top}Aw + \\gamma m - e^{\\top}y\n",
    "\\end{bmatrix}}_{(n+1) \\times 1}\n",
    "\\qquad\n",
    "{\\mathbf{\\nabla^2 f(x)} = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial w^2} & \\frac{\\partial^2 f}{\\partial w \\gamma} \\\\\n",
    "\\frac{\\partial^2 f}{\\partial \\gamma w} & \\frac{\\partial^2 f}{\\partial \\gamma^2}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix} A^{\\top}A & A^{\\top}e \\\\\n",
    "e^{\\top}A & m\n",
    "\\end{bmatrix}}_{(n+1) \\times (n+1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{h_i(x)} = \\|w\\|_2^2 = w^{\\top}w \\in \\mathbb R\n",
    "\\qquad \\qquad\n",
    "{\\mathbf{\\nabla h_i(x)} = \\begin{bmatrix} \\frac{\\partial h}{\\partial w} \\\\\n",
    "\\frac{\\partial h}{\\partial \\gamma}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix} 2w \\\\\n",
    "0\n",
    "\\end{bmatrix}}_{(n+1) \\times 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{\\mathbf{\\nabla^2 h_i(x)} = \\begin{bmatrix} \\frac{\\partial^2 h}{\\partial w^2} & \\frac{\\partial^2 h}{\\partial w \\gamma} \\\\\n",
    "\\frac{\\partial^2 h}{\\partial \\gamma w} & \\frac{\\partial^2 h}{\\partial \\gamma^2}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix} 2 & 0 & \\dotsb & 0 & 0 \\\\\n",
    "0 & 2 & & & 0 \\\\\n",
    "\\vdots & & \\ddots & & \\vdots \\\\\n",
    "0 & & & 2 & 0 \\\\\n",
    "0 & 0 & \\dotsb & 0 & 0 \\\\\n",
    "\\end{bmatrix}}_{(n+1) \\times (n+1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implementation and test\n",
    " \n",
    "> Implementation code and test with the `death_rate` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries needed for optimization.\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import LinearConstraint\n",
    "from scipy.optimize import Bounds\n",
    "from scipy.optimize import NonlinearConstraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset file.\n",
    "data = np.loadtxt(\"death_rate.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = data[:,:-1]     # Matrix of explanatory variables.    m·n\n",
    "y = data[:,-1]      # Vector of the response variable.    m·1\n",
    "\n",
    "m = (A.shape)[0]    # Number of rows of matrix A.\n",
    "n = (A.shape)[1]    # Number of columns of matrix A.\n",
    "\n",
    "w = np.zeros(n)     # Vector of weights.                  n·1\n",
    "gamma = 0           # Intercept parameter.                1·1\n",
    "e = np.ones(m)      # Vector of ones.                     m·1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' OBJECTIVE FUNCTION f(x) '''\n",
    "def obje_f(x):\n",
    "    # 'x' is a vector containing:    the vector of weights 'w'\n",
    "    #                                the intercept parameter 'gamma'\n",
    "    w = x[:-1]\n",
    "    gamma = x[-1]\n",
    "    \n",
    "    # Returns the evaluation of the objective function f(x).    1·1\n",
    "    form = np.matmul(A,w) + gamma*e - y    # Aw + ge - y\n",
    "    return(1/2 * np.dot(form,form))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' GRADIENT OF THE OBJECTIVE FUNCTION ∇f(x) '''\n",
    "grad = np.zeros(n+1)    # Initialization of the gradient vector.\n",
    "\n",
    "def grad_f(x):\n",
    "    # 'x' is a vector containing:    the vector of weights 'w'\n",
    "    #                                the intercept parameter 'gamma'\n",
    "    w = x[:-1]\n",
    "    gamma = x[-1]\n",
    "    \n",
    "    # Returns the evaluation of the gradient vector ∇f(x).    (n+1)·1\n",
    "    grad[:-1] = np.matmul(np.transpose(A),(np.matmul(A,w) + gamma*e - y))           # A'(Aw + ge - y)\n",
    "    grad[-1] = np.matmul(np.transpose(e),np.matmul(A,w)) + gamma*m - np.dot(e,y)    # e'Aw + gm - e'y\n",
    "    return(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' HESSIAN OF THE OBJECTIVE FUNCTION ∇^(2)f(x) '''\n",
    "hess = np.zeros((n+1,n+1))                    # Initialization of the hessian matrix.\n",
    "hess[:n,:n] = np.matmul(np.transpose(A),A)    # Computation of the first 'n·n' inputs.    A'*A\n",
    "hess[:n,-1] = np.matmul(np.transpose(A),e)    # Computation of the 'n·(n+1)' column.      A'*e\n",
    "hess[-1,:n] = np.transpose(hess[:n,-1])       # Computation of the '(n+1)·n' row.         e'*A\n",
    "hess[-1,-1] = m                               # Computation of the '(n+1)·(n+1)' element.   m\n",
    "\n",
    "def hess_f(x):\n",
    "    # Returns the evaluation of the hessian matrix ∇^(2)f(x).    (n+1)·(n+1)\n",
    "    return(hess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' CONSTRAINT CONDITION h(x) '''\n",
    "def cons_f(x):\n",
    "    # 'x' is a vector containing:    the vector of weights 'w'\n",
    "    #                                the intercept parameter 'gamma'\n",
    "    w = x[:-1]\n",
    "    \n",
    "    # Returns the evaluation of the constraint condition h(x).    1·1\n",
    "    return(np.dot(w,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' JACOBIAN OF THE CONSTRAINT ∇h(x) '''\n",
    "jaco = np.zeros(n+1)    # Initialization of the jacobian matrix.\n",
    "\n",
    "def cons_J(x):\n",
    "    # 'x' is a vector containing:    the vector of weights 'w'\n",
    "    #                                the intercept parameter 'gamma'\n",
    "    w = x[:-1]\n",
    "    \n",
    "    # Returns the evaluation of the jacobian matrix ∇h(x).    (n+1)·1\n",
    "    jaco[:-1] = 2*w\n",
    "    return(jaco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' HESSIAN OF THE CONSTRAINT ∇h^(2)(x) '''\n",
    "hessC = 2*np.eye(n+1)\n",
    "hessC[-1,-1] = 0         # Initialization of the hessian matrix.\n",
    "\n",
    "def cons_H(x, v):\n",
    "    # Returns the evaluation of the sum of Lagrange multipliers by the hessian matrix.    1·1\n",
    "    return(v[0]*hessC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the nonlinear constraint (with the hiperparameter 't').\n",
    "t = 1.0\n",
    "nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf, t, jac = cons_J, hess = cons_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`xtol` termination condition is satisfied.\n",
      "Number of iterations: 123, function evaluations: 182, CG iterations: 617, optimality: 1.92e-04, constraint violation: 0.00e+00, execution time: 0.22 s.\n",
      " barrier_parameter: 2.048000000000001e-09\n",
      " barrier_tolerance: 2.048000000000001e-09\n",
      "          cg_niter: 617\n",
      "      cg_stop_cond: 2\n",
      "            constr: [array([1.])]\n",
      "       constr_nfev: [182]\n",
      "       constr_nhev: [65]\n",
      "       constr_njev: [59]\n",
      "    constr_penalty: 1.0\n",
      "  constr_violation: 0.0\n",
      "    execution_time: 0.2233867645263672\n",
      "               fun: 61484.654640764944\n",
      "              grad: array([-10531.43189764,   1000.60086433,  -2221.25312436,    813.32934107,\n",
      "         -105.85284749,    731.60168261,   5517.35190524,   -121.83830841,\n",
      "       -14014.33632358,   2722.5970384 ,  -4603.2434718 ,   4482.96567861,\n",
      "        -2369.32573194,  -8124.22726336,   -214.83474213,     -0.00000456])\n",
      "               jac: [array([[ 0.97647914, -0.09277617,  0.20595561, -0.07541226,  0.00981472,\n",
      "        -0.06783444, -0.51157139,  0.0112969 ,  1.29941563, -0.25244043,\n",
      "         0.42681484, -0.41566263,  0.21968498,  0.75328204,  0.01991958,\n",
      "         0.        ]])]\n",
      "   lagrangian_grad: array([-0.00014326, -0.00003118,  0.0001211 ,  0.00012179, -0.0000665 ,\n",
      "       -0.00001314, -0.00013913, -0.00000032, -0.00006218,  0.00005154,\n",
      "        0.00015817, -0.00011087,  0.00019184, -0.00001733,  0.00004808,\n",
      "       -0.00000456])\n",
      "           message: '`xtol` termination condition is satisfied.'\n",
      "            method: 'tr_interior_point'\n",
      "              nfev: 182\n",
      "              nhev: 59\n",
      "               nit: 123\n",
      "             niter: 123\n",
      "              njev: 59\n",
      "        optimality: 0.0001918447196658235\n",
      "            status: 2\n",
      "           success: True\n",
      "         tr_radius: 1.0000000000000005e-09\n",
      "                 v: [array([10785.10672666])]\n",
      "                 x: array([  0.48823957,  -0.04638809,   0.10297781,  -0.03770613,\n",
      "         0.00490736,  -0.03391722,  -0.2557857 ,   0.00564845,\n",
      "         0.64970782,  -0.12622022,   0.21340742,  -0.20783131,\n",
      "         0.10984249,   0.37664102,   0.00995979, 895.14138573])\n"
     ]
    }
   ],
   "source": [
    "x0 = np.zeros(n+1)    # Definition of the starting point.\n",
    "\n",
    "sol = minimize(obje_f, x0, method = 'trust-constr', jac = grad_f, hess = hess_f,    # Solve problem.\n",
    "               constraints = [nonlinear_constraint], options = {'verbose': 1})\n",
    "\n",
    "np.set_printoptions(suppress = True)\n",
    "print(sol)            # Display of the obtained solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With the implementation above we get a successful result. It takes only $123$ iterations and $182$ function evaluations to converge with a total execution time of **0.21 seconds**.\n",
    "\n",
    "> Firstly, regarding optimization conditions it should be noted that:\n",
    "1. There is no constraint violation\n",
    "2. The values of the Lagrangian gradient are, in practice, zero\n",
    "\n",
    "> In order to interpret the results better, we consider the sufficient optimality conditions:\n",
    "\n",
    "> The point $x^*$ is local minimizer if: *First-order conditions (KKT)*\n",
    "1. $h(x^*)=0$, $g(x^*) \\leq 0$\n",
    "2. $\\nabla_x L(x^*,\\lambda^*,\\mu^*) = \\nabla f(x*)+\\nabla h(x^*)\\lambda^*+\\nabla g(x^*)\\mu^*=0$\n",
    "3. $\\mu^* \\geq 0$ and ${\\mu^*}^T g(x^*)=0$ $\\quad$ (if \n",
    "$g_j(x^*)$ is inactive then $\\mu_j^*=0$)\n",
    "\n",
    "> *Second-order conditions*\n",
    "4. $d^T \\nabla_{xx}^2 L(x^*,\\lambda^*,\\mu^*) d > 0$, for all $d \\in M'=\\{d: \\nabla h(x^*)^T d = 0, \\nabla g_j(x^*)^T d = 0, j\\in A(x^*) \\cap \\{j: \\mu_j^* > 0 \\} \\}$\n",
    "\n",
    "> As ridge regression is a convex optimization problem, we only need to check the first-order conditions (KKT), because the property 4 is satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.009315019397718e-11"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Condition 1.\n",
    "cons_f(sol.x) - t # <-- g(x*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - As we do not have $h(x^*)=0$ restriccion, we only have to evaluate $g(x^*) \\leq 0$. Indeed, the condition is satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00014326, -0.00003118,  0.0001211 ,  0.00012179, -0.0000665 ,\n",
       "       -0.00001314, -0.00013913, -0.00000032, -0.00006218,  0.00005154,\n",
       "        0.00015817, -0.00011087,  0.00019184, -0.00001733,  0.00004808,\n",
       "       -0.00000456])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Condition 2.\n",
    "sol.lagrangian_grad # <-- ∇L(x*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - The values of the Lagrangian gradient are, in practice, zero. Thus, the condition is satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([10785.10672666])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-3.2455783658349524e-07"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Condition 3.\n",
    "print(sol.v) # <-- mu\n",
    "(sol.v[0][0])*(cons_f(sol.x)-1) # <-- mu·g(x*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - The value of $\\mu^*$ is greater than zero and the product of ${\\mu^*}^T g(x^*)$ is, in practice, zero. Thus, the condition is satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Taking all the above into account, we can conclude that our results are an optimal solution to the proposed problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Comparison with AMPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With the optimization software AMPL we get the following results for the `death_rate` dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\n",
    "\\text{MINOS 5.51: optimal solution found.} \\\\\n",
    "\\text{133 iterations, objective 61484.65464} \\\\\n",
    "\\text{Nonlin evals: obj = 361, grad = 360, constrs = 361, Jac = 360.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\n",
    "\\text{w [*] :=} \\\\\n",
    "1.\\text{ 0.48824} \\\\\n",
    "2.\\text{ -0.0463881} \\\\\n",
    "3.\\text{ 0.102978} \\\\\n",
    "4.\\text{ -0.0377061} \\\\\n",
    "5.\\text{ 0.00490736} \\\\\n",
    "6.\\text{ -0.0339172} \\\\\n",
    "7.\\text{ -0.255786}\\\\ \n",
    "8.\\text{ 0.00564845} \\\\\n",
    "9.\\text{ 0.649708} \\\\\n",
    "10.\\text{ -0.12622} \\\\\n",
    "11.\\text{ 0.213407} \\\\\n",
    "12.\\text{ -0.207831} \\\\ \n",
    "13.\\text{ 0.109842} \\\\\n",
    "14.\\text{ 0.376641} \\\\\n",
    "15.\\text{ 0.00995978;} \\\\\n",
    "\\text{gamma = 895.141} \\\\\n",
    "\\text{norm2_w = -10785.1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The first remark we observe is that we get the exact same results with AMPL for the optimal solution. Comparing the values of the objective function, the weights, the gamma and the norm, we observe no differences regarding our implementation.\n",
    "\n",
    "> A curious fact is that, although AMPL performs a couple of iterations more than our implementation, it obtains a better time when it comes to finding the optimal solution (due to the internal resolution algorithms) with only *0.009984* seconds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
